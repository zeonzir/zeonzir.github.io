<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Home - Madan Ravi Ganesh</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Madan Ravi Ganesh</h1>
        <div class="avatar">
		    <img src="images/Profile.jpg">
	    </div>
        <p><center>Computer Vision Ph.D. candidate at the University of Michigan</center></p>
        <p><strong>Advisor:</strong> Jason J. Corso</p>
        <p><strong>Co-Advisor:</strong> Salimeh Yasaei Sekeh</p>
        <h3>Contact me here:</h3>
        <ul class="contact_urls">
            <li><a target="_blank" href="https://github.com/zeonzir">
                    <svg class="favicons" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px"
                    width="24" height="24"
                    viewBox="0 0 24 24"
                    style=" fill:#000000;">
                <path d="M10.9,2.1c-4.6,0.5-8.3,4.2-8.8,8.7c-0.5,4.7,2.2,8.9,6.3,10.5C8.7,21.4,9,21.2,9,20.8v-1.6c0,0-0.4,0.1-0.9,0.1 c-1.4,0-2-1.2-2.1-1.9c-0.1-0.4-0.3-0.7-0.6-1C5.1,16.3,5,16.3,5,16.2C5,16,5.3,16,5.4,16c0.6,0,1.1,0.7,1.3,1c0.5,0.8,1.1,1,1.4,1 c0.4,0,0.7-0.1,0.9-0.2c0.1-0.7,0.4-1.4,1-1.8c-2.3-0.5-4-1.8-4-4c0-1.1,0.5-2.2,1.2-3C7.1,8.8,7,8.3,7,7.6C7,7.2,7,6.6,7.3,6 c0,0,1.4,0,2.8,1.3C10.6,7.1,11.3,7,12,7s1.4,0.1,2,0.3C15.3,6,16.8,6,16.8,6C17,6.6,17,7.2,17,7.6c0,0.8-0.1,1.2-0.2,1.4 c0.7,0.8,1.2,1.8,1.2,3c0,2.2-1.7,3.5-4,4c0.6,0.5,1,1.4,1,2.3v2.6c0,0.3,0.3,0.6,0.7,0.5c3.7-1.5,6.3-5.1,6.3-9.3 C22,6.1,16.9,1.4,10.9,2.1z"></path></svg> GitHub</a></li>
            <li><a href="mailto:madantrg@umich.edu"><img class="favicons" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABmJLR0QA/wD/AP+gvaeTAAAAtElEQVRIie3TQQpBURjF8R9migUo2YJSliEjS7AFW7AFWzAxsAqlrIGBOWVGz8CVF89zEym9U6de3z3v/Ou7XQr9WqXUd/KN7vKHSx+UBehh/UbXFoO8QOK2pirGOKbmz3zCBLWMnkzAHK0wa2ORU75CN2QbmMYAEhwwQsVljUPsXpzv7zpyAVcv0QnnTcyCm2HWCZmsf6MA6R3XU9mYO4oGXL1BP3gTkce/PrQC8GeAQr/XGQz+Xbwnnbx7AAAAAElFTkSuQmCC"> Email</a></li>

            <li><a href="docs/madantrg_cv.pdf"><img class="favicons" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABmJLR0QA/wD/AP+gvaeTAAAATElEQVRIiWNgGOqAEYvYf2qayUShYQQBzS1gISCPLQixAZzBOuA+oDTCB94HsDj4j8ZHBwMXB0M/o43GAUEzR+OAIBj6cUBzC4Y+AAATxAwulfy0/AAAAABJRU5ErkJggg=="> Curriculum Vitae</a></li>
        </ul>
      </header>

      <section>
        <h2>Conversation starters</h2>
        <p  style="text-align:justify"> <strong> Deep learning's application to computer vision:</strong> As the true "black box" approach of modern machine learning, I like works that pick apart the inner workings of artificial neural networks to find the root cause of their behaviour. I believe that systematically highlighting what ANNs can and CANNOT do is critical. </p>

        <p  style="text-align:justify"> <strong>Analysis and Development of video-based architectures:</strong> While the underpinning of image-based deep learning has received a lot of attention over the years, such work have not made the leap to video. In making this leap across time, a new dimesionality of possibilities and associated problems have arisen. Hence, my focus has been on fully characterizing neural network architectures w.r.t. their ability to handle time and develop ANNs that utilize all of the available information in videos. </p>

        <p  style="text-align:justify"> <strong>Efficient memory usage in deep learning:</strong> With ANN datasets and parameters spanning millions and billions in number, their applicability in real-world scenarios, often with restrictions on available memory or required throughput, is restricted. In trying to solve this disparity, I emphasize on methods that help reduce the storage/computation of activations, deep network compression, and other relevant areas.</p>

        </section>

        <section>
        <h2>Research Projects</h2>
        <h3> LILAC: Learning with Incremental Labels and Adaptive Compensation</h3>
        <p style="text-align:justify"> Common curriculum learning schemes impose "difficulty" as a metric to samples of a dataset and gradually increases the scale of difficult samples shown to a learner. In LILAC, we approach curriculum learning from the perspective that all labels are equally important and instead gradually introduce labels to the learner. Further, regularizing a learner by adaptively modifying the target vector for mis-classified samples helps further improve a learner's performance.<br /> <a target="_blank" href="https://arxiv.org/pdf/2001.04529">[PDF]</a> <a target="_blank" href="https://github.com/MichiganCOG/LILAC">[Code]</a> </p> 
        <img src="images/lilac_main.png">
        <br />

        <h3> A Geometric Online Adaptive approach to OSFS</h3>
        <p style="text-align:justify"> Feature selection is one of many common and simple approaches to dimensionality reduction. However, until recently prior works have focused on streaming feature selection(OSFS), with the assumption that data from all samples remain available throughout, as the closest approximation to real-world problems. In GOA, we introduce the Online Streaming Feature Selection with Streaming Samples (OSFS-SS) as a natural extension to OSFS, where both samples and features are streamed. To solve OSFS-SS and its simpler subset OSFS, we introduce the Geometric Online Adaptive(GOA) approach which currently acheives state-of-the-art performance on both these areas while using an equivalent or lower number of features than existing methods. <br /> <a target="_blank" href="https://arxiv.org/abs/1910.01182">[PDF]</a> <a target="_blank" href="https://github.com/MichiganCOG/GOA">[Code]</a> </p> 
        <img src="images/goa_main.png">
        <br />

        <h3> Temporal Blocks Dataset: An empirical analysis of CNNs' temporal modelling capabilities</h3>
        <p style="text-align:justify"> The evolution of CNNs from image-based designs to video-centric architectures has marked a tremendous improvement in performance over applications liek activity recognition, video summarization and object tracking. However, a critical element of this evolution is their ability to handle the temporal dimension. Using controlled synthetic baseline, we try actively characterizing different CNN architectures' ability to capture time in videos. We highlight the core functionalities and deficiencies of each deep network model by testing for direction of time, spatiotemporal motion, memory decay and dataset bias.<br /> <a target="_blank" href="https://arxiv.org/">[Broken PDF]</a> <a target="_blank" href="https://github.com/MichiganCOG/">[Broken Code]</a> </p> 
        <img src="images/tbd_main.png">
        <br />

        <h3> T-RECS: Training for Rate-Invariance Embedding by Controlling Speed</h3>
        <p style="text-align:justify"> CNNs are not robust in handling speed variations in input videos, an important security flaw. In T-RECS, we design a simple preprocessing methodology to improve robustness to input speed variations across multiple CNN architectures. Further, in analyzing different CNN architectures, we clearly observe the influence that atomic CNN modules like 3D convolution, LSTMs and others, have on their ability to handle speed variations.<br /> <a target="_blank" href="https://arxiv.org/abs/1803.08094">[PDF]</a> <a target="_blank" href="https://github.com/MichiganCOG/T-RECS">[Code]</a> </p> 
        <img src="images/t_recs_main.png">

        <h2>Software Projects</h2>
        <h3>ViP: Video Platform for Recognition and Detection in PyTorch</h3>
        <p  style="text-align:justify">The ever expanding domain of video-based deep learning contains a number of distinct problem branches like action recognition, object tracking, and many more. By developing a video-specific platform that could help leverage ideas from multiple such branches, we believe that research can expand beyond one problem domain. Hence, we deployed a pytorch-based video platform that can handle any image- or video-based problem domain with minimal changes. It includes strong bookkeeping, mimics large mini-batch computations on low memory systems while including a large suite of video-specific preprocessing functions. <br/> <a target="_blank" href="https://arxiv.org/pdf/1910.02793.pdf">[PDF]</a> <a target="_blank" href="https://github.com/MichiganCOG/ViP">[Code]</a> </p>
        <img src="images/vip_main.png">
        <br />      
        <h3>M-PACT: Michigan Platform for Activity Classification in Tensorflow</h3>
        <p  style="text-align:justify"> In the interest of deploying an activity recognition framework to aid reproducible research, quick prototyping and reduce time consumed by unnecessary pipeline development we developed M-PACT. It contains a unique video-specific preprocessing pipeline that allows the end-user to request a variety of clip combinations from videos with a small number of arguments.<br /> <a target="_blank" href="https://arxiv.org/abs/1804.05879">[PDF]</a> <a target="_blank" href="https://github.com/MichiganCOG/M-PACT">[Code]</a> </p>
        <img src="images/m_pact_main.png">
      </section>

  </body>
</html>
